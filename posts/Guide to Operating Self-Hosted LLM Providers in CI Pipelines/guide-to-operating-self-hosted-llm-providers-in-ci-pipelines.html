<!doctype html>

<html lang="en">

<head>
	<meta charset="utf-8">

	<title>kianjones.com - Post</title>
	<meta name="kianjones.com - Post" content="The personal site of Kian Jones">

	<link rel="stylesheet" href="./../../style.css">

</head>

<body>

	<nav>
		<a href="./../../resources.html">Resources</a>
		<a href="./../../posts.html">Posts</a>
		<a href="./../../index.html">Home</a>
		<span id="prompt">root@KianOS:~/posts$</span>
		<span id="cursor">‚ñä</span>

	</nav>

	<div class="content">
		<h1 class="page-title">Guide to Operating Self-Hosted LLM Providers in CI Pipelines</h1>
		<article class="page sans">
			<div class="page-body">
				<p>At Letta, there are a ton of interesting and novel challenges to solve. One such challenge is the slew of LLM inference providers a modern, pluggable framework must accommodate, not to mention their level of adherence to published standards <em>*cough*</em> OpenAI Chat Completions <em>*cough*</em>. With many models across many providers, it can be easy to accidentally break a provider integration or drop support for a particular model.</p>

				<p>The conventional software engineering solution to this problem is to do integration testing, or to add "CI checks", in today's DevOps vernacular.</p>

				<p>In Letta's CI, we do routine and extensive testing to ensure we do not drop support for any of our <strong>hosted</strong> inference providers in what we call our <em>model-sweep</em> ‚Ñ¢Ô∏è (Blog post on that coming soon üëÄ) whose results are currently <a href="https://docs.letta.com/connecting-model-providers/supported-models">published here</a>.</p>

				<p>But what about <em>self-hosted</em> inference providers?</p>

				<p>These are much trickier. Not only are they more varied and bespoke, but they also pose a new challenge over the standard hosted providers: Freedom to choose whatever model you want!</p>

				<p>The freedom to choose is a great benefit and compelling argument for bootstrapping self-hosted providers, but when it comes to building CI jobs which involve LLMs, you need to add as much determinism as possible. <strong>Trust me.</strong> Some of the non-deterministic challenges we've had to face are beyond just asserting the model returns certain text, or that said text is in a particular format or schema. Letta as a framework abstracts a ton of complex functionalities from the upstream models, for example, tool calling. A smaller/faster/cheaper model, or even a huge one which is heavily quantized, may choose to call the wrong tool, or maybe not call one at all!</p>

				<p>Challenges like this make software testing harder the closer you get to the model, but Letta is open-source + self-hostable, and we value our OSS community highly. It's important we ship high-quality software, so let's add self-hosted provider testing to CI!</p>

				<p>First things first, what is the scope of the project? Although we'd love to support every possible provider of LLM inference, let's be realistic. In this case, we will limit our CI testing to the 3 largest self-hosted providers:</p>
				<ul>
					<li><a href="https://lmstudio.ai/">LM Studio</a> (or to use with <a href="https://docs.letta.com/guides/server/providers/ollama">Letta</a>)</li>
					<li><a href="https://ollama.com/">Ollama</a> (or to use with <a href="https://docs.letta.com/guides/server/providers/lmstudio">Letta</a>)</li>
					<li><a href="https://docs.vllm.ai/en/latest/">vLLM</a> (or to use with <a href="https://docs.letta.com/guides/server/providers/vllm">Letta</a>)</li>
				</ul>

				<p>If you are unacquainted with these providers, here's a quick overview: LM Studio is a user-friendly desktop app with a beautiful UI, Ollama is a collection of CLI tools with a model registry, and vLLM is the hyperscaler-preferred inference engine optimized for high-throughput and low-latency serving of transformer models.</p>

				<p>If you'd prefer an analogy, I like to think of LM Studio as a Mac, Ollama as Docker, and vLLM as Linux. Hopefully you'll understand why by the end of this post.</p>

				<h2>Our Self-hosted CI Runners</h2>

				<p>So let's talk a bit about our design choices. To minimize the burden of setup (we are a startup after all üòâ) we are going to run each provider as a GCP VM configured as a self-hosted Github Actions runner. This allows us to easily integrate it into CI (leveraging existing self-hosted runners I might make a follow up post about), as well as mitigate the need to expose any endpoints unnecessarily beyond the machine running the test, i.e. keeping the provider accessible over localhost only.</p>

				<p>So here's our setup:</p>
				<p><img src="Pasted image 20250802104301.png" alt="Setup diagram 1"><img src="Pasted image 20250802104325.png" alt="Setup diagram 2"></p>

				<p>And here's how our tests will be defined at the top-level:</p>
				<pre><code>runner: '["self-hosted", "gpu", "ollama"]'
  matrix-strategy: |
    {
      "fail-fast": false,
      "matrix": {
        "test_suite": [
          "test_providers.py::test_ollama",
          "integration_test_send_message.py"
        ]
      }
    }</code></pre>

        <h2>LM Studio - Sleek and Configurable</h2>
        <p>When I started writing this post, I had a lot of trouble running LM Studio in CI because it was only runnable as a full desktop application, which required a display server in the background, attaching the app to said display server, etc. which as you may imagine wasn't robust enough for the tumultuous world of best-effort, ephemeral CI runners, subject to autoscaling and public cloud GPU contention. Thankfully the team at LM Studio was able to get me a preview release of their <a href="https://hub.docker.com/r/lmstudio/llmster-preview">new docker container</a>!</p>

        <p>Here's what I worked out for our LM Studio Setup:</p>

        <pre><code>#!/bin/bash
set -e

echo "Setting up LMStudio..."

LLM_DIR="/opt/llm-providers"
sudo mkdir -p $LLM_DIR/lmstudio
sudo chown -R ${RUNNER_USER:-ci-runner}:${RUNNER_USER:-ci-runner} $LLM_DIR

# Create models directory for persistent storage
sudo mkdir -p $LLM_DIR/lmstudio/models
sudo chown -R ${RUNNER_USER:-ci-runner}:${RUNNER_USER:-ci-runner} $LLM_DIR/lmstudio/models

# Pull LMStudio Docker image and pre-download model
echo "Pulling LMStudio Docker image..."
sudo docker pull lmstudio/llmster-preview:cpu

echo "Setting up LMStudio Docker Compose configuration..."
sudo tee $LLM_DIR/lmstudio/docker-compose.yml > /dev/null << EOF
version: '3.8'
services:
  lmstudio:
    image: lmstudio/llmster-preview:cpu
    ports:
      - "1234:1234"
    volumes:
      - $LLM_DIR/lmstudio/models:/root/.lmstudio
    restart: unless-stopped
EOF

# Create startup script that starts container and loads model
sudo tee $LLM_DIR/lmstudio/start-lmstudio.sh > /dev/null << 'EOF'
#!/bin/bash
cd /opt/llm-providers/lmstudio
docker compose up -d
sleep 30  # Wait for container to be ready (15 seconds wasn't long enough)
docker exec lmstudio-lmstudio-1 lms get qwen2.5-7@Q4_K_M --yes
docker exec lmstudio-lmstudio-1 lms load qwen2.5-7b-instruct-1m
EOF

sudo chmod +x $LLM_DIR/lmstudio/start-lmstudio.sh

# Create systemd service for LMStudio
sudo cat << EOF | sudo tee /etc/systemd/system/lmstudio.service
[Unit]
Description=LMStudio Self-hosted Provider
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
User=${RUNNER_USER:-ci-runner}
WorkingDirectory=$LLM_DIR/lmstudio
ExecStart=$LLM_DIR/lmstudio/start-lmstudio.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl start lmstudio.service
sudo systemctl enable lmstudio.service

echo "LMStudio setup complete!"</code></pre>

        <p>Some of the feedback I gave the LM Studio team was around making their LMS container more ergonomic for the automations for which it's built. In this case, the <code>lms load</code> command has to be run before chat completions requests can start being served, which means we should just codify all these prerequisite commands into a script called by the systemd unit since even if it was called at startup, if the service fails after startup we would similarly have to load the model upon service restart. Can't wait to see what the LM Studio team cooks up once their container is officially released!</p>

        <h2>Ollama - Simply Works</h2>

        <p>Ollama looks similar, but with more constrained model choice (has to be listed on Ollama registry, unlike vLLM and LM Studio which support pulling from hugging face) and the extremely convenient JIT model loading which loads the model into VRAM at inference time, and expires it once a TTL is reached. This is super convenient, because as long as the model is downloaded, as I was going back and forth trying different models, I could just naively query chat completions endpoint and expect the response from the requested model!</p>

        <p>The gift and curse of Ollama is their registry. It allows you easily pull models like docker containers, like <code>ollama pull qwen3:32b</code>, trying to determine the appropriate size/quantization of model to pull. On the flip side, you are limited only to which are in the Ollama registry, and furthermore I didn't see a way to download a specific quantization if you wanted to override their calculations.</p>

        <p>Although Ollama has a docker container, when I Claude Coded up a scaffold for this project, this just kinda worked out of the box! ü§∑</p>

        <pre><code>#!/bin/bash
set -e

echo "Setting up Ollama..."

# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Configure Ollama to bind to all interfaces for container access
sudo mkdir -p /etc/systemd/system/ollama.service.d
cat << EOF | sudo tee /etc/systemd/system/ollama.service.d/override.conf
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
EOF

sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama

# Wait for Ollama to be ready
sleep 10

# Pre-pull qwen2.5:7b for testing
echo "Pre-pulling qwen2.5:7b..."
ollama pull qwen2.5:7b

echo "Ollama setup complete!"</code></pre>

        <p>This worked pretty well, and didn't require any last minute/JIT on-start hooks, which was super convenient.</p>

        <h2>vLLM - Powerful Yet Daunting</h2>

        <p>vLLM is in stark contrast to the previous two providers, in that its target market is hyperscalers and powerusers. vLLM gives you, as the expressions goes, "enough rope to hang yourself." When getting started, their configuration and customization options are overwhelming. Based on my previous CI runner setups I put together, I tried to jump in a immediately hit a blocker: I had accidentally downloaded Qwen3-32b unquantized, whose >60GB did not fit in my puny, cost-effective 2 x L4 GPU runner üòÖ.</p>

				<p>I quickly changed to Qwen3-32b-AWQ, and added the necessary edits to my claude code scaffold (<code>--tensor-parallel-size 2</code>) and added use of the nvidia container runtime:</p>
				<pre><code>    runtime: nvidia
    ports:
      - "8000:8000"
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]</code></pre>
				<p>with the corresponding system-level change to the docker daemon.json file:</p>
				<pre><code>{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "storage-driver": "overlay2",
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "mtu": 1460
}</code></pre>

        <p>Our final vLLM service looks like this:</p>
        <pre><code>#!/bin/bash
set -e

echo "Setting up vLLM..."

LLM_DIR="/opt/llm-providers"
sudo mkdir -p $LLM_DIR/vllm
sudo chown -R ${RUNNER_USER:-ci-runner}:${RUNNER_USER:-ci-runner} $LLM_DIR

# Setup VLLM with Docker Compose
echo "Pulling vLLM Docker image..."
sudo docker pull vllm/vllm-openai:latest

echo "Setting up VLLM configuration..."
sudo tee $LLM_DIR/vllm/docker-compose.yml > /dev/null << EOF
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8000:8000"
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      --model "Qwen/Qwen3-32B-AWQ"
      --tensor-parallel-size 2
      --host 0.0.0.0
      --port 8000
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
EOF

# Create systemd service for vLLM
sudo cat << EOF | sudo tee /etc/systemd/system/vllm.service
[Unit]
Description=vLLM Self-hosted Provider
After=docker.service
Requires=docker.service

[Service]
Type=simple
User=${RUNNER_USER:-ci-runner}
WorkingDirectory=$LLM_DIR/vllm
ExecStart=docker compose up
ExecStop=docker compose down
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable vllm.service

echo "vLLM setup complete!"</code></pre>

				<h2>End Results</h2>

				<p>After all this, we can now run our per-PR CI tests against our self-hosted providers and ensure we do not regress or break compatibility for LM Studio, Ollama, and vLLM!</p>
				<p><img src="Pasted image 20250819002429.png" alt="CI Results 1"><img src="Pasted image 20250819002344.png" alt="CI Results 2"></p>
			</div>
		</article>
	</div>

</body>

</html>